{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/tensorflow-deep-learning/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTWetPM7AWfY"
   },
   "source": [
    "# 05. Transfer Learning with TensorFlow Part 2: Fine-tuning\n",
    "\n",
    "In the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food Vision project than building our own models (even with less data).\n",
    "\n",
    "Now we're going to cover another type of transfer learning: fine-tuning.\n",
    "\n",
    "In **fine-tuning transfer learning** the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data.\n",
    "\n",
    "For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-transfer-learning-feature-extraction-vs-fine-tuning.png)\n",
    "*Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective.*\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "We're going to go through the follow with TensorFlow:\n",
    "\n",
    "- Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data\n",
    "- Using the Keras Functional API (a differnt way to build models in Keras)\n",
    "- Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)\n",
    "- Data augmentation (how to make your training dataset more diverse without adding more data)\n",
    "- Running a series of modelling experiments on our Food Vision data\n",
    "  - Model 0: a transfer learning model using the Keras Functional API\n",
    "  - Model 1: a feature extraction transfer learning model on 1% of the data with data augmentation\n",
    "  - Model 2: a feature extraction transfer learning model on 10% of the data with data augmentation\n",
    "  - Model 3: a fine-tuned transfer learning model on 10% of the data\n",
    "  - Model 4: a fine-tuned transfer learning model on 100% of the data\n",
    "- Introduce the ModelCheckpoint callback to save intermediate training results\n",
    "- Compare model experiments results using TensorBoard\n",
    "\n",
    "## How you can use this notebook\n",
    "\n",
    "You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.\n",
    "\n",
    "Write all of the code yourself.\n",
    "\n",
    "Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?\n",
    "\n",
    "You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.\n",
    "\n",
    "Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to **write more code**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nq4kxIpQMpZT",
    "outputId": "8248e560-36dc-4570-ceac-a962b6151cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 11 18:35:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.88                 Driver Version: 580.88         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "|  0%   43C    P8             19W /  324W |     834MiB /  16376MiB |     19%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2880    C+G   ...we\\Microsoft.Media.Player.exe      N/A      |\n",
      "|    0   N/A  N/A            3336    C+G   ...lus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A            5500    C+G   ...crosoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A            7768    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A            9412    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A            9596    C+G   ...ekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "|    0   N/A  N/A           10024    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           10684    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           11772    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           14220    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           14940    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           15212    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           15596    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A           15860    C+G   ...em_tray\\lghub_system_tray.exe      N/A      |\n",
      "|    0   N/A  N/A           17280    C+G   ...26wp6bftszj\\TranslucentTB.exe      N/A      |\n",
      "|    0   N/A  N/A           18264    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18988    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Are we using a GPU? (if not & you're using Google Colab, go to Runtime -> Change Runtime Type -> Harware Accelerator: GPU )\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_t7iZxXvlZ9"
   },
   "source": [
    "## Creating helper functions\n",
    "\n",
    "Throughout your machine learning experiments, you'll likely come across snippets of code you want to use over and over again.\n",
    "\n",
    "For example, a plotting function which plots a model's `history` object (see `plot_loss_curves()` below).\n",
    "\n",
    "You could recreate these functions over and over again.\n",
    "\n",
    "But as you might've guessed, rewritting the same functions becomes tedious.\n",
    "\n",
    "One of the solutions is to store them in a helper script such as [`helper_functions.py`](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py). And then import the necesary functionality when you need it.\n",
    "\n",
    "For example, you might write:\n",
    "\n",
    "```\n",
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "...\n",
    "\n",
    "plot_loss_curves(history)\n",
    "```\n",
    "\n",
    "Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEuiAtXkhsJM",
    "outputId": "c2ca862e-a4f8-4bd4-8a24-f015df35742e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Get helper_functions.py script from course GitHub\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py \n",
    "\n",
    "# Import helper functions we're going to use\n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZUvqGMxizYc"
   },
   "source": [
    "Wonderful, now we've got a bunch of helper functions we can use throughout the notebook without having to rewrite them from scratch each time.\n",
    "\n",
    "> 🔑 **Note:** If you're running this notebook in Google Colab, when it times out Colab will delete the `helper_functions.py` file. So to use the functions imported above, you'll have to rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWemvyC2pv3T"
   },
   "source": [
    "## 10 Food Classes: Working with less data\n",
    "\n",
    "We saw in the [previous notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb) that we could get great results with only 10% of the training data using transfer learning with TensorFlow Hub.\n",
    "\n",
    "In this notebook, we're going to continue to work with smaller subsets of the data, except this time we'll have a look at how we can use the in-built pretrained models within the `tf.keras.applications` module as well as how to fine-tune them to our own custom dataset.\n",
    "\n",
    "We'll also practice using a new but similar dataloader function to what we've used before, [`image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) which is part of the [`tf.keras.preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing) module.\n",
    "\n",
    "Finally, we'll also be practicing using the [Keras Functional API](https://keras.io/guides/functional_api/) for building deep learning models. The Functional API is a more flexible way to create models than the tf.keras.Sequential API.\n",
    "\n",
    "We'll explore each of these in more detail as we go.\n",
    "\n",
    "Let's start by downloading some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7m6pjH0JTyJ",
    "outputId": "1877d951-5772-4d4a-9b2c-e1b35713cb28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '10_food_classes_10_percent.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get 10% of the data of the 10 classes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43munzip_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10_food_classes_10_percent.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\python\\Python3_TF_Certificate\\helper_functions.py:243\u001b[0m, in \u001b[0;36munzip_data\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munzip_data\u001b[39m(filename):\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m  Unzips filename into the current working directory.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    filename (str): a filepath to a target zip folder to be unzipped.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m   zip_ref \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m   zip_ref\u001b[38;5;241m.\u001b[39mextractall()\n\u001b[0;32m    245\u001b[0m   zip_ref\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\zipfile.py:1239\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1241\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '10_food_classes_10_percent.zip'"
     ]
    }
   ],
   "source": [
    "# Get 10% of the data of the 10 classes\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip \n",
    "\n",
    "unzip_data(\"10_food_classes_10_percent.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fknY0DqM14EO"
   },
   "source": [
    "The dataset we're downloading is the 10 food classes dataset (from Food 101) with 10% of the training images we used in the previous notebook.\n",
    "\n",
    "> 🔑 **Note:** You can see how this dataset was created in the [image data modification notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8IHQ3i9s2lRm",
    "outputId": "4311188c-1d7c-4716-b7dd-04305dc5de96"
   },
   "outputs": [],
   "source": [
    "# Walk through 10 percent data directory and list number of files\n",
    "walk_through_dir(\"10_food_classes_10_percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4bhxd-z2Vov"
   },
   "source": [
    "We can see that each of the training directories contain 75 images and each of the testing directories contain 250 images.\n",
    "\n",
    "Let's define our training and test filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tELTNwq6L43a"
   },
   "outputs": [],
   "source": [
    "# Create training and test directories\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkEZyNnLzppR"
   },
   "source": [
    "Now we've got some image data, we need a way of loading it into a TensorFlow compatible format.\n",
    "\n",
    "Previously, we've used the [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class. And while this works well and is still very commonly used, this time we're going to use the `image_data_from_directory` function.\n",
    "\n",
    "It works much the same way as `ImageDataGenerator`'s `flow_from_directory` method meaning your images need to be in the following file format:\n",
    "\n",
    "```\n",
    "Example of file structure\n",
    "\n",
    "10_food_classes_10_percent <- top level folder\n",
    "└───train <- training images\n",
    "│   └───pizza\n",
    "│   │   │   1008104.jpg\n",
    "│   │   │   1638227.jpg\n",
    "│   │   │   ...      \n",
    "│   └───steak\n",
    "│       │   1000205.jpg\n",
    "│       │   1647351.jpg\n",
    "│       │   ...\n",
    "│   \n",
    "└───test <- testing images\n",
    "│   └───pizza\n",
    "│   │   │   1001116.jpg\n",
    "│   │   │   1507019.jpg\n",
    "│   │   │   ...      \n",
    "│   └───steak\n",
    "│       │   100274.jpg\n",
    "│       │   1653815.jpg\n",
    "│       │   ...    \n",
    "```\n",
    "\n",
    "One of the main benefits of using [`tf.keras.prepreprocessing.image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) rather than `ImageDataGenerator` is that it creates a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object rather than a generator. The main advantage of this is the `tf.data.Dataset` API is much more efficient (faster) than the `ImageDataGenerator` API which is paramount for larger datasets.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcBs8C5wJ0et",
    "outputId": "0dbce4fd-5b92-4eb3-8688-f4b1b5e0f22d"
   },
   "outputs": [],
   "source": [
    "# Create data inputs\n",
    "import tensorflow as tf\n",
    "IMG_SIZE = (224, 224) # define image size\n",
    "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
    "                                                                            image_size=IMG_SIZE,\n",
    "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
    "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
    "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
    "                                                                           image_size=IMG_SIZE,\n",
    "                                                                           label_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaNOwJXu5Kms"
   },
   "source": [
    "Wonderful! Looks like our dataloaders have found the correct number of images for each dataset.\n",
    "\n",
    "For now, the main parameters we're concerned about in the `image_dataset_from_directory()` funtion are:\n",
    "* `directory` - the filepath of the target directory we're loading images in from.\n",
    "* `image_size` - the target size of the images we're going to load in (height, width).\n",
    "* `batch_size` - the batch size of the images we're going to load in. For example if the `batch_size` is 32 (the default), batches of 32 images and labels at a time will be passed to the model.\n",
    "\n",
    "There are more we could play around with if we needed to [in the `tf.keras.preprocessing` documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory).\n",
    "\n",
    "If we check the training data datatype we should see it as a `BatchDataset` with shapes relating to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a30VfDiv3ZGc",
    "outputId": "536f060f-a82a-4d76-ae6f-d2c48ac0464c"
   },
   "outputs": [],
   "source": [
    "# Check the training data datatype\n",
    "train_data_10_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7p3h9nC6ZH1"
   },
   "source": [
    "In the above output:\n",
    "\n",
    "* `(None, 224, 224, 3)` refers to the tensor shape of our images where `None` is the batch size, `224` is the height (and width) and `3` is the color channels (red, green, blue).\n",
    "* `(None, 10)` refers to the tensor shape of the labels where `None` is the batch size and `10` is the number of possible labels (the 10 different food classes).\n",
    "* Both image tensors and labels are of the datatype `tf.float32`.\n",
    "\n",
    "The `batch_size` is `None` due to it only being used during model training. You can think of `None` as a placeholder waiting to be filled with the `batch_size` parameter from `image_dataset_from_directory()`.\n",
    "\n",
    "Another benefit of using the `tf.data.Dataset` API are the assosciated methods which come with it.\n",
    "\n",
    "For example, if we want to find the name of the classes we were working with, we could use the `class_names` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mRe2rnv7YdW",
    "outputId": "ada12547-900a-41a9-fc07-5c2a97e9f366"
   },
   "outputs": [],
   "source": [
    "# Check out the class names of our dataset\n",
    "train_data_10_percent.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c09o0KAr8SFM"
   },
   "source": [
    "Or if we wanted to see an example batch of data, we could use the `take()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZ25UPow7q5K",
    "outputId": "1e1ec9b5-de59-4160-9b7c-b248a509696f"
   },
   "outputs": [],
   "source": [
    "# See an example batch of data\n",
    "for images, labels in train_data_10_percent.take(1):\n",
    "  print(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1WcrMG58YtV"
   },
   "source": [
    "Notice how the image arrays come out as tensors of pixel values where as the labels come out as one-hot encodings (e.g. `[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]` for `hamburger`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5DL4UYE8m1j"
   },
   "source": [
    "### Model 0: Building a transfer learning model using the Keras Functional API\n",
    "\n",
    "Alright, our data is tensor-ified, let's build a model.\n",
    "\n",
    "To do so we're going to be using the [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) module as it contains a series of already trained (on ImageNet) computer vision models as well as the Keras Functional API to construct our model.\n",
    "\n",
    "We're going to go through the following steps:\n",
    "\n",
    "1. Instantiate a pre-trained base model object by choosing a target model such as [`EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0) from `tf.keras.applications`, setting the `include_top` parameter to `False` (we do this because we're going to create our own top, which are the output layers for the model).\n",
    "2. Set the base model's `trainable` attribute to `False` to freeze all of the weights in the pre-trained model.\n",
    "3. Define an input layer for our model, for example, what shape of data should our model expect?\n",
    "4. [Optional] Normalize the inputs to our model if it requires. Some computer vision models such as `ResNetV250` require their inputs to be between 0 & 1. \n",
    "\n",
    "> 🤔 **Note:** As of writing, the `EfficientNet` models in the `tf.keras.applications` module do not require images to be normalized (pixel values between 0 and 1) on input, where as many of the other models do. I posted [an issue to the TensorFlow GitHub](https://github.com/tensorflow/tensorflow/issues/42506) about this and they confirmed this. \n",
    "\n",
    "5. Pass the inputs to the base model.\n",
    "6. Pool the outputs of the base model into a shape compatible with the output activation layer (turn base model output tensors into same shape as label tensors). This can be done using [`tf.keras.layers.GlobalAveragePooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) or [`tf.keras.layers.GlobalMaxPooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D?hl=en) though the former is more common in practice.\n",
    "7. Create an output activation layer using `tf.keras.layers.Dense()` with the appropriate activation function and number of neurons.\n",
    "8. Combine the inputs and outputs layer into a model using [`tf.keras.Model()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n",
    "9. Compile the model using the appropriate loss function and choose of optimizer.\n",
    "10. Fit the model for desired number of epochs and with necessary callbacks (in our case, we'll start off with the TensorBoard callback).\n",
    "\n",
    "Woah... that sounds like a lot. Before we get ahead of ourselves, let's see it in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ayNga8DsJLPa",
    "outputId": "e585c86c-d376-4ace-a746-dc8a87d7dd91"
   },
   "outputs": [],
   "source": [
    "# 1. Create base model with tf.keras.applications\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "# 2. Freeze the base model (so the pre-learned patterns remain)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 3. Create inputs into the base model\n",
    "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
    "\n",
    "# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNet\n",
    "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "\n",
    "# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNet inputs don't have to be normalized)\n",
    "x = base_model(inputs)\n",
    "# Check data shape after passing it to base_model\n",
    "print(f\"Shape after base_model: {x.shape}\")\n",
    "\n",
    "# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "print(f\"After GlobalAveragePooling2D(): {x.shape}\")\n",
    "\n",
    "# 7. Create the output activation layer\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "# 8. Combine the inputs with the outputs into a model\n",
    "model_0 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 9. Compile the model\n",
    "model_0.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 10. Fit the model (we use less steps for validation so it's faster)\n",
    "history_10_percent = model_0.fit(train_data_10_percent,\n",
    "                                 epochs=5,\n",
    "                                 steps_per_epoch=len(train_data_10_percent),\n",
    "                                 validation_data=test_data_10_percent,\n",
    "                                 # Go through less of the validation data so epochs are faster (we want faster experiments!)\n",
    "                                 validation_steps=int(0.25 * len(test_data_10_percent)), \n",
    "                                 # Track our model's training logs for visualization later\n",
    "                                 callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_feature_extract\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUhkvBx0Hb8D"
   },
   "source": [
    "Nice! After a minute or so of training our model performs incredibly well on both the training (87%+ accuracy) and test sets (~83% accuracy).\n",
    "\n",
    "This is incredible. All thanks to the power of transfer learning.\n",
    "\n",
    "It's important to note the kind of transfer learning we used here is called feature extraction transfer learning, similar to what we did with the TensorFlow Hub models.\n",
    "\n",
    "In other words, we passed our custom data to an already pre-trained model (`EfficientNetB0`), asked it \"what patterns do you see?\" and then put our own output layer on top to make sure the outputs were tailored to our desired number of classes. \n",
    "\n",
    "We also used the Keras Functional API to build our model rather than the Sequential API. For now, the benefits of this main not seem clear but when you start to build more sophisticated models, you'll probably want to use the Functional API. So it's important to have exposure to this way of building models.\n",
    "\n",
    "> 📖 **Resource:** To see the benefits and use cases of the Functional API versus the Sequential API, check out the [TensorFlow Functional API documentation](https://www.tensorflow.org/guide/keras/functional).\n",
    "\n",
    "Let's inspect the layers in our model, we'll start with the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUuQOyKv4_27",
    "outputId": "df5cca5e-2358-47ae-d1d5-7c14200c1a96"
   },
   "outputs": [],
   "source": [
    "# Check layers in our base model\n",
    "for layer_number, layer in enumerate(base_model.layers):\n",
    "  print(layer_number, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVTG-ZZUJhKB"
   },
   "source": [
    "Wow, that's a lot of layers... to handcode all of those would've taken a fairly long time to do, yet we can still take advatange of them thanks to the power of transfer learning.\n",
    "\n",
    "How about a summary of the base model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gKubJmUu91N",
    "outputId": "68436c34-39d6-4883-c722-ccaa5b4e25fe"
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZyIF_Zlu63C"
   },
   "source": [
    "You can see how each of the different layers have a certain number of parameters each. Since we are using a pre-trained model, you can think of all of these parameters are patterns the base model has learned on another dataset. And because we set `base_model.trainable = False`, these patterns remain as they are during training (they're frozen and don't get updated).\n",
    "\n",
    "Alright that was the base model, let's see the summary of our overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGY1BjnzJDEx",
    "outputId": "67952c6a-3b30-4296-95d0-259e3abd3854"
   },
   "outputs": [],
   "source": [
    "# Check summary of model constructed with Functional API\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmoVfJqgKoY5"
   },
   "source": [
    "Our overall model has five layers but really, one of those layers (`efficientnetb0`) has 236 layers.\n",
    "\n",
    "You can see how the output shape started out as `(None, 224, 224, 3)` for the input layer (the shape of our images) but was transformed to be `(None, 10)` by the output layer (the shape of our labels), where `None` is the placeholder for the batch size.\n",
    "\n",
    "Notice too, the only trainable parameters in the model are those in the output layer.\n",
    "\n",
    "How do our model's training curves look?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "tWYqK_86xSUJ",
    "outputId": "816526b9-c38f-4319-8a8e-2e1846aa87cb"
   },
   "outputs": [],
   "source": [
    "# Check out our model's training curves\n",
    "plot_loss_curves(history_10_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBNakPyq3OyC"
   },
   "source": [
    "## Getting a feature vector from a trained model\n",
    "\n",
    "> 🤔 **Question:** What happens with the `tf.keras.layers.GlobalAveragePooling2D()` layer? I haven't seen it before.\n",
    "\n",
    "The [`tf.keras.layers.GlobalAveragePooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) layer transforms a 4D tensor into a 2D tensor by averaging the values across the inner-axes.\n",
    "\n",
    "The previous sentence is a bit of a mouthful, so let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYNLE0VU_c50",
    "outputId": "1a13f29e-39bc-4d5b-fe42-415282f8e3a0"
   },
   "outputs": [],
   "source": [
    "# Define input tensor shape (same number of dimensions as the output of efficientnetb0)\n",
    "input_shape = (1, 4, 4, 3)\n",
    "\n",
    "# Create a random tensor\n",
    "tf.random.set_seed(42)\n",
    "input_tensor = tf.random.normal(input_shape)\n",
    "print(f\"Random input tensor:\\n {input_tensor}\\n\")\n",
    "\n",
    "# Pass the random tensor through a global average pooling 2D layer\n",
    "global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n",
    "print(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")\n",
    "\n",
    "# Check the shapes of the different tensors\n",
    "print(f\"Shape of input tensor: {input_tensor.shape}\")\n",
    "print(f\"Shape of 2D global averaged pooled input tensor: {global_average_pooled_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq5RAKrENCpT"
   },
   "source": [
    "You can see the `tf.keras.layers.GlobalAveragePooling2D()` layer condensed the input tensor from shape `(1, 4, 4, 3)` to `(1, 3)`. It did so by averaging the `input_tensor` across the middle two axes.\n",
    "\n",
    "We can replicate this operation using the `tf.reduce_mean()` operation and specifying the appropriate axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBzN5HfAAUHx",
    "outputId": "bf06dcab-5432-42fb-a649-f3b0153591cc"
   },
   "outputs": [],
   "source": [
    "# This is the same as GlobalAveragePooling2D()\n",
    "tf.reduce_mean(input_tensor, axis=[1, 2]) # average across the middle axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urQv2FtCPS85"
   },
   "source": [
    "Doing this not only makes the output of the base model compatible with the input shape requirement of our output layer (`tf.keras.layers.Dense()`), it also condenses the information found by the base model into a lower dimension **feature vector**.\n",
    "\n",
    "> 🔑 **Note:** One of the reasons feature extraction transfer learning is named how it is is because what often happens is a pretrained model outputs a **feature vector** (a long tensor of numbers, in our case, this is the output of the [`tf.keras.layers.GlobalAveragePooling2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) layer) which can then be used to extract patterns out of.\n",
    "\n",
    "> 🛠 **Practice:** Do the same as the above cell but for [`tf.keras.layers.GlobalMaxPool2D()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DafB65YULbRe"
   },
   "source": [
    "## Running a series of transfer learning experiments\n",
    "\n",
    "We've seen the incredible results of transfer learning on 10% of the training data, what about 1% of the training data?\n",
    "\n",
    "What kind of results do you think we can get using 100x less data than the original CNN models we built ourselves?\n",
    "\n",
    "Why don't we answer that question while running the following modelling experiments:\n",
    "1. `model_1`: Use feature extraction transfer learning on 1% of the training data with data augmentation.\n",
    "2. `model_2`: Use feature extraction transfer learning on 10% of the training data with data augmentation.\n",
    "3. `model_3`: Use fine-tuning transfer learning on 10% of the training data with data augmentation.\n",
    "4. `model_4`: Use fine-tuning transfer learning on 100% of the training data with data augmentation.\n",
    "\n",
    "While all of the experiments will be run on different versions of the training data, they will all be evaluated on the same test dataset, this ensures the results of each experiment are as comparable as possible.\n",
    "\n",
    "All experiments will be done using the `EfficientNetB0` model within the `tf.keras.applications` module.\n",
    "\n",
    "To make sure we're keeping track of our experiments, we'll use our `create_tensorboard_callback()` function to log all of the model training logs.\n",
    "\n",
    "We'll construct each model using the Keras Functional API and instead of implementing data augmentation in the `ImageDataGenerator` class as we have previously, we're going to build it right into the model using the [`tf.keras.layers.experimental.preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) module.\n",
    "\n",
    "Let's begin by downloading the data for experiment 1, using feature extraction transfer learning on 1% of the training data with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2P1rLJmaCXZN",
    "outputId": "0630e683-7dbd-41ad-9f8b-be4fdebb0738"
   },
   "outputs": [],
   "source": [
    "# Download and unzip data\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
    "unzip_data(\"10_food_classes_1_percent.zip\")\n",
    "\n",
    "# Create training and test dirs\n",
    "train_dir_1_percent = \"10_food_classes_1_percent/train/\"\n",
    "test_dir = \"10_food_classes_1_percent/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hnXOA1EoyRF"
   },
   "source": [
    "How many images are we working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sqoi0htuo6Sb",
    "outputId": "b1e9104a-39b8-4634-e60c-b09d7ccbdb11"
   },
   "outputs": [],
   "source": [
    "# Walk through 1 percent data directory and list number of files\n",
    "walk_through_dir(\"10_food_classes_1_percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEC70hlwo_mB"
   },
   "source": [
    "Alright, looks like we've only got seven images of each class, this should be a bit of a challenge for our model.\n",
    "\n",
    "> 🔑 **Note:** As with the 10% of data subset, the 1% of images were chosen at random from the original full training dataset. The test images are the same as the ones which have previously been used. If you want to see how this data was preprocessed, check out the [Food Vision Image Preprocessing notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb).\n",
    "\n",
    "Time to load our images in as `tf.data.Dataset` objects, to do so, we'll use the [`image_dataset_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Llh9sHmdtgf6",
    "outputId": "5b4b9938-8d4a-4714-cdc7-ce0c837cc471"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "IMG_SIZE = (224, 224)\n",
    "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,\n",
    "                                                                           label_mode=\"categorical\",\n",
    "                                                                           batch_size=32, # default\n",
    "                                                                           image_size=IMG_SIZE)\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D4po3S-qNJ-"
   },
   "source": [
    "Data loaded. Time to augment it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl8lSdTjDTCV"
   },
   "source": [
    "### Adding data augmentation right into the model\n",
    "\n",
    "Previously we've used the different parameters of the `ImageDataGenerator` class to augment our training images, this time we're going to build data augmentation right into the model.\n",
    "\n",
    "How?\n",
    "\n",
    "Using the [`tf.keras.layers.experimental.preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) module and creating a dedicated data augmentation layer.\n",
    "\n",
    "This a relatively new feature added to TensorFlow 2.2+ but it's very powerful. Adding a data augmentation layer to the model has the following benefits:\n",
    "* Preprocessing of the images (augmenting them) happens on the GPU rather than on the CPU (much faster).\n",
    "  * Images are best preprocessed on the GPU where as text and structured data are more suited to be preprocessed on the CPU.\n",
    "* Image data augmentation only happens during training so we can still export our whole model and use it elsewhere. And if someone else wanted to train the same model as us, including the same kind of data augmentation, they could.\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-data-augmentation-inside-a-model.png)\n",
    "*Example of using data augmentation as the first layer within a model (EfficientNetB0).*\n",
    "\n",
    "> 🤔 **Note:** At the time of writing, the preprocessing layers we're using for data augmentation are in *experimental* status within the in TensorFlow library. This means although the layers should be considered stable, the code may change slightly in a future version of TensorFlow. For more information on the other preprocessing layers avaiable and the different methods of data augmentation, check out the [Keras preprocessing layers guide](https://keras.io/guides/preprocessing_layers/) and the [TensorFlow data augmentation guide](https://www.tensorflow.org/tutorials/images/data_augmentation).\n",
    "\n",
    "To use data augmentation right within our model we'll create a Keras Sequential model consisting of only data preprocessing layers, we can then use this Sequential model within another Functional model.\n",
    "\n",
    "If that sounds confusing, it'll make sense once we create it in code.\n",
    "\n",
    "The data augmentation transformations we're going to use are:\n",
    "* [RandomFlip](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomFlip) - flips image on horizontal or vertical axis.\n",
    "* [RandomRotation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomRotation) - randomly rotates image by a specified amount.\n",
    "* [RandomZoom](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomZoom) - randomly zooms into an image by specified amount.\n",
    "* [RandomHeight](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomHeight) - randomly shifts image height by a specified amount.\n",
    "* [RandomWidth](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomWidth) - randomly shifts image width by a specified amount.\n",
    "* [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling) - normalizes the image pixel values to be between 0 and 1, this is worth mentioning because it is required for some image models but since we're using the `tf.keras.applications` implementation of `EfficientNetB0`, it's not required.\n",
    "\n",
    "There are more option but these will do for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjHAalnakGEu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "data_augmentation = keras.Sequential([\n",
    "  preprocessing.RandomFlip(\"horizontal\"),\n",
    "  preprocessing.RandomRotation(0.2),\n",
    "  preprocessing.RandomZoom(0.2),\n",
    "  preprocessing.RandomHeight(0.2),\n",
    "  preprocessing.RandomWidth(0.2),\n",
    "  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0\n",
    "], name =\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkeE62wxuyXG"
   },
   "source": [
    "And that's it! Our data augmentation Sequential model is ready to go. As you'll see shortly, we'll be able to slot this \"model\" as a layer into our transfer learning model later on.\n",
    "\n",
    "But before we do that, let's test it out by passing random images through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "VBWXrlJXwKkE",
    "outputId": "ccd983e7-9bff-4540-8ec7-0d00e26506fc"
   },
   "outputs": [],
   "source": [
    "# View a random image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import random\n",
    "target_class = random.choice(train_data_1_percent.class_names) # choose a random class\n",
    "target_dir = \"10_food_classes_1_percent/train/\" + target_class # create the target directory\n",
    "random_image = random.choice(os.listdir(target_dir)) # choose a random image from target directory\n",
    "random_image_path = target_dir + \"/\" + random_image # create the choosen random image path\n",
    "img = mpimg.imread(random_image_path) # read in the chosen target image\n",
    "plt.imshow(img) # plot the target image\n",
    "plt.title(f\"Original random image from class: {target_class}\")\n",
    "plt.axis(False); # turn off the axes\n",
    "\n",
    "# Augment the image\n",
    "augmented_img = data_augmentation(tf.expand_dims(img, axis=0)) # data augmentation model requires shape (None, height, width, 3)\n",
    "plt.figure()\n",
    "plt.imshow(tf.squeeze(augmented_img)/255.) # requires normalization after augmentation\n",
    "plt.title(f\"Augmented random image from class: {target_class}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8UwsBz1xF_H"
   },
   "source": [
    "Run the cell above a few times and you can see the different random augmentations on different classes of images. Because we're going to add the data augmentation model as a layer in our upcoming transfer learning model, it'll apply these kind of random augmentations to each of the training images which passes through it. \n",
    "\n",
    "Doing this will make our training dataset a little more varied. You can think of it as if you were taking a photo of food in real-life, not all of the images are going to be perfect, some of them are going to be orientated in strange ways. These are the kind of images we want our model to be able to handle.\n",
    "\n",
    "Speaking of model, let's build one with the Functional API. We'll run through all of the same steps as before except for one difference, we'll add our data augmentation Sequential model as a layer immediately after the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koH2FCIaAVe9"
   },
   "source": [
    "## Model 1: Feature extraction transfer learning on 1% of the data with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbj-Z7Bhut-Y",
    "outputId": "b6fb03ce-c99e-496e-9b67-dd8c504238e4"
   },
   "outputs": [],
   "source": [
    "# Setup input shape and base model, freezing the base model layers\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create input layer\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "# Add in data augmentation Sequential model as a layer\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Give base_model inputs (after augmentation) and don't train it\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Pool output features of base model\n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "\n",
    "# Put a dense layer on as the output\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "# Make a model with inputs and outputs\n",
    "model_1 = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "history_1_percent = model_1.fit(train_data_1_percent,\n",
    "                    epochs=5,\n",
    "                    steps_per_epoch=len(train_data_1_percent),\n",
    "                    validation_data=test_data,\n",
    "                    validation_steps=int(0.25* len(test_data)), # validate for less steps\n",
    "                    # Track model training logs\n",
    "                    callbacks=[create_tensorboard_callback(\"transfer_learning\", \"1_percent_data_aug\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svhrIqgCi27C"
   },
   "source": [
    "Wow! How cool is that? Using only 7 training images per class, using transfer learning our model was able to get ~40% accuracy on the validation set. This result is pretty amazing since the [original Food-101 paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf) achieved 50.67% accuracy with all the data, namely, 750 training images per class (**note:** this metric was across 101 classes, not 10, we'll get to 101 classes soon).\n",
    "\n",
    "If we check out a summary of our model, we should see the data augmentation layer just after the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lloN15WdPkgb",
    "outputId": "e79a8aa2-44c5-4ba7-b6bc-54844e4d24f3"
   },
   "outputs": [],
   "source": [
    "# Check out model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPwjk5CAj9cZ"
   },
   "source": [
    "There it is. We've now got data augmentation built right into the our model. This means if we saved it and reloaded it somewhere else, the data augmentation layers would come with it.\n",
    "\n",
    "The important thing to remember is **data augmentation only runs during training**. So if we were to evaluate or use our model for inference (predicting the class of an image) the data augmentation layers will be automatically turned off.\n",
    "\n",
    "To see this in action, let's evaluate our model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgWXERdy8nXP",
    "outputId": "8c18041b-6107-4d1c-bc54-59e2bc189a8a"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test data\n",
    "results_1_percent_data_aug = model_1.evaluate(test_data)\n",
    "results_1_percent_data_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OABaDYcokefk"
   },
   "source": [
    "The results here may be slightly better/worse than the log outputs of our model during training because during training we only evaluate our model on 25% of the test data using the line `validation_steps=int(0.25 * len(test_data))`. Doing this speeds up our epochs but still gives us enough of an idea of how our model is going.\n",
    "\n",
    "Let's stay consistent and check out our model's loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "ShCcuguspbyz",
    "outputId": "e45a0cd2-5ce5-4266-fdde-6fabfc91a6a9"
   },
   "outputs": [],
   "source": [
    "# How does the model go with a data augmentation layer with 1% of data\n",
    "plot_loss_curves(history_1_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEPFBiDPk-ig"
   },
   "source": [
    "It looks like the metrics on both datasets would improve if we kept training for more epochs. But we'll leave that for now, we've got more experiments to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cd5jPoPC2Ug"
   },
   "source": [
    "## Model 2: Feature extraction transfer learning with 10% of data and data augmentation\n",
    "\n",
    "Alright, we've tested 1% of the training data with data augmentation, how about we try 10% of the data with data augmentation?\n",
    "\n",
    "But wait...\n",
    "\n",
    "> 🤔 **Question:** How do you know what experiments to run?\n",
    "\n",
    "Great question. \n",
    "\n",
    "The truth here is you often won't. Machine learning is still a very experimental practice. It's only after trying a fair few things that you'll start to develop an intuition of what to try.\n",
    "\n",
    "My advice is to follow your curiosity as tenaciously as possible. If you feel like you want to try something, write the code for it and run it. See how it goes. The worst thing that'll happen is you'll figure out what doesn't work, the most valuable kind of knowledge.\n",
    "\n",
    "From a practical standpoint, as we've talked about before, you'll want to reduce the amount of time between your initial experiments as much as possible. In other words, run a plethora of smaller experiments, using less data and less training iterations before you find something promising and then scale it up.\n",
    "\n",
    "In the theme of scale, let's scale our 1% training data augmentation experiment up to 10% training data augmentation. That sentence doesn't really make sense but you get what I mean.\n",
    "\n",
    "We're going to run through the exact same steps as the previous model, the only difference being using 10% of the training data instead of 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv75Z8BkuTmy"
   },
   "outputs": [],
   "source": [
    "# Get 10% of the data of the 10 classes (uncomment if you haven't gotten \"10_food_classes_10_percent.zip\" already)\n",
    "# !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "# unzip_data(\"10_food_classes_10_percent.zip\")\n",
    "\n",
    "train_dir_10_percent = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Vcnuoorminm"
   },
   "source": [
    "Data downloaded. Let's create the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eZ4SzbiC9zX",
    "outputId": "b2fe4146-8bce-46eb-daa8-036cde10a909"
   },
   "outputs": [],
   "source": [
    "# Setup data inputs\n",
    "import tensorflow as tf\n",
    "IMG_SIZE = (224, 224)\n",
    "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n",
    "                                                                            label_mode=\"categorical\",\n",
    "                                                                            image_size=IMG_SIZE)\n",
    "# Note: the test data is the same as the previous experiment, we could\n",
    "# skip creating this, but we'll leave this here to practice.\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlHIzDo5mz5L"
   },
   "source": [
    "Awesome! We've got 10x more images to work with, 75 per class instead of 7 per class.\n",
    "\n",
    "Let's build a model with data augmentation built in. We could reuse the data augmentation Sequential model we created before but we'll recreate it to practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuuP1FWADHfc"
   },
   "outputs": [],
   "source": [
    "# Create a functional model with data augmentation\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build data augmentation layer\n",
    "data_augmentation = Sequential([\n",
    "  preprocessing.RandomFlip('horizontal'),\n",
    "  preprocessing.RandomHeight(0.2),\n",
    "  preprocessing.RandomWidth(0.2),\n",
    "  preprocessing.RandomZoom(0.2),\n",
    "  preprocessing.RandomRotation(0.2),\n",
    "  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet                 \n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "# Setup the input shape to our model\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Create a frozen base model\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create input and output layers\n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\") # create input layer\n",
    "x = data_augmentation(inputs) # augment our training images\n",
    "x = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don't get updated: https://keras.io/guides/transfer_learning/#build-a-model \n",
    "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001), # use Adam optimizer with base learning rate\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RiP2aR6nqyK"
   },
   "source": [
    "### Creating a ModelCheckpoint callback\n",
    "\n",
    "Our model is compiled and ready to be fit, so why haven't we fit it yet?\n",
    "\n",
    "Well, for this experiment we're going to introduce a new callback, the `ModelCheckpoint` callback.\n",
    "\n",
    "The [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback gives you the ability to save your model, as a whole in the [`SavedModel`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) format or the [weights (patterns) only](https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights) to a specified directory as it trains. \n",
    "\n",
    "This is helpful if you think your model is going to be training for a long time and you want to make backups of it as it trains. It also means if you think your model could benefit from being trained for longer, you can reload it from a specific checkpoint and continue training from there.\n",
    "\n",
    "For example, say you fit a feature extraction transfer learning model for 5 epochs and you check the training curves and see it was still improving and you want to see if fine-tuning for another 5 epochs could help, you can load the checkpoint, unfreeze some (or all) of the base model layers and then continue training.\n",
    "\n",
    "In fact, that's exactly what we're going to do. \n",
    "\n",
    "But first, let's create a `ModelCheckpoint` callback. To do so, we have to specifcy a directory we'd like to save to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11M6B736noly"
   },
   "outputs": [],
   "source": [
    "# Setup checkpoint path\n",
    "checkpoint_path = \"ten_percent_model_checkpoints_weights/checkpoint.ckpt\" # note: remember saving directly to Colab is temporary\n",
    "\n",
    "# Create a ModelCheckpoint callback that saves the model's weights only\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True, # set to False to save the entire model\n",
    "                                                         save_best_only=False, # set to True to save only the best model instead of a model every epoch \n",
    "                                                         save_freq=\"epoch\", # save every epoch\n",
    "                                                         verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9T4MdK1nynZ"
   },
   "source": [
    "> 🤔 **Question:** What's the difference between saving the entire model (SavedModel format) and saving the weights only?\n",
    "\n",
    "The [`SavedModel`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) format saves a model's architecture, weights and training configuration all in one folder. It makes it very easy to reload your model exactly how it is elsewhere. However, if you do not want to share all of these details with others, you may want to save and share the weights only (these will just be large tensors of non-human interpretable numbers). If disk space is an issue, saving the weights only is faster and takes up less space than saving the whole model.\n",
    "\n",
    "Time to fit the model.\n",
    "\n",
    "Because we're going to be fine-tuning it later, we'll create a variable `initial_epochs` and set it to 5 to use later.\n",
    "\n",
    "We'll also add in our `checkpoint_callback` in our list of `callbacks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOgMGhi0nx0u",
    "outputId": "f6c6167e-1766-4867-9183-a08e1810efb4"
   },
   "outputs": [],
   "source": [
    "# Fit the model saving checkpoints every epoch\n",
    "initial_epochs = 5\n",
    "history_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
    "                                          epochs=initial_epochs,\n",
    "                                          validation_data=test_data,\n",
    "                                          validation_steps=int(0.25 * len(test_data)), # do less steps per validation (quicker)\n",
    "                                          callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_data_aug\"), \n",
    "                                                     checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elbLc46m1kAz"
   },
   "source": [
    "Would you look at that! Looks like our `ModelCheckpoint` callback worked and our model saved its weights every epoch without too much overhead (saving the whole model takes longer than just the weights).\n",
    "\n",
    "Let's evaluate our model and check its loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KBwXaDwvlEX",
    "outputId": "5944515d-755e-48dd-b381-870b9a769730"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test data\n",
    "results_10_percent_data_aug = model_2.evaluate(test_data)\n",
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "W-TeYMc_Hh0p",
    "outputId": "83741585-8e32-45b8-9718-71a2cfbd8b65"
   },
   "outputs": [],
   "source": [
    "# Plot model loss curves\n",
    "plot_loss_curves(history_10_percent_data_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-yHYHnx2GZE"
   },
   "source": [
    "Looking at these, our model's performance with 10% of the data and data augmentation isn't as good as the model with 10% of the data without data augmentation (see `model_0` results above), however the curves are trending in the right direction, meaning if we decided to train for longer, its metrics would likely improve.\n",
    "\n",
    "Since we checkpointed (is that a word?) our model's weights, we might as well see what it's like to load it back in. We'll be able to test if it saved correctly by evaluting it on the test data.\n",
    "\n",
    "To load saved model weights you can use the the [`load_weights()`](https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_options) method, passing it the path where your saved weights are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yo5uxm9Du_kO",
    "outputId": "6656e469-d0c0-4d04-bf50-3d902cbddf58"
   },
   "outputs": [],
   "source": [
    "# Load in saved model weights and evaluate model\n",
    "model_2.load_weights(checkpoint_path)\n",
    "loaded_weights_model_results = model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYTrmgeSgRkc"
   },
   "source": [
    "Now let's compare the results of our previously trained model and the loaded model. These results should very close if not exactly the same. The reason for minor differences comes down to the precision level of numbers calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oF-zsM_wMVQ",
    "outputId": "9fca1ee8-875c-4e6a-ec74-a22b63a79cc4"
   },
   "outputs": [],
   "source": [
    "# If the results from our native model and the loaded weights are the same, this should output True\n",
    "results_10_percent_data_aug == loaded_weights_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2noU9rlgflw"
   },
   "source": [
    "If the above cell doesn't output `True`, it's because the numbers are close but not the *exact* same (due to how computers store numbers with degrees of precision).\n",
    "\n",
    "However, they should be *very* close..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0u4N8g9mgBI0",
    "outputId": "102bf07f-7280-49fe-ea8f-16ee53bbd268"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Check to see if loaded model results are very close to native model results (should output True)\n",
    "np.isclose(np.array(results_10_percent_data_aug), np.array(loaded_weights_model_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6srnISjohK8N",
    "outputId": "de0374d9-c922-4b6b-88e2-692f6f255653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1920929e-07  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Check the difference between the two results\n",
    "print(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_RYtaqD0Mv6"
   },
   "source": [
    "## Model 3: Fine-tuning an existing model on 10% of the data\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-fine-tuning-an-efficientnet-model.png)\n",
    "*High-level example of fine-tuning an EfficientNet model. Bottom layers (layers closer to the input data) stay frozen where as top layers (layers closer to the output data) are updated during training.*\n",
    "\n",
    "So far our saved model has been trained using feature extraction transfer learning for 5 epochs on 10% of the training data and data augmentation.\n",
    "\n",
    "This means all of the layers in the base model (EfficientNetB0) were frozen during training.\n",
    "\n",
    "For our next experiment we're going to switch to fine-tuning transfer learning. This means we'll be using the same base model except we'll be unfreezing some of its layers (ones closest to the top) and running the model for a few more epochs.\n",
    "\n",
    "The idea with fine-tuning is to start customizing the pre-trained model more to our own data.\n",
    "\n",
    "> 🔑 **Note:** Fine-tuning usually works best *after* training a feature extraction model for a few epochs and with large amounts of data. For more on this, check out [Keras' guide on Transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).\n",
    "\n",
    "We've verified our loaded model's performance, let's check out its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfsHnkOIrx9K",
    "outputId": "23395730-9cee-4cb2-d687-e68f0742a45f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fceca1c9208>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fceca1c90b8>,\n",
       " <tensorflow.python.keras.engine.functional.Functional at 0x7fcebe5bc630>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x7fcebe62b8d0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fcebe5f1e80>]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layers in loaded model\n",
    "model_2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpzkIVuqtbXC",
    "outputId": "caf47dff-456f-480f-841d-621fbd815f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for layer in model_2.layers:\n",
    "  print(layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TR7Ym5UN5tou"
   },
   "source": [
    "Looking good. We've got an input layer, a Sequential layer (the data augmentation model), a Functional layer (EfficientNetB0), a pooling layer and a Dense layer (the output layer).\n",
    "\n",
    "How about a summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w55Z7qFYsUfy",
    "outputId": "a3489f33-abe3-474a-d08c-fa64bb8dcdca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "data_augmentation (Sequentia (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n",
      "_________________________________________________________________\n",
      "global_average_pooling_layer (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 10)                12810     \n",
      "=================================================================\n",
      "Total params: 4,062,381\n",
      "Trainable params: 12,810\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSAz6_oi62Sn"
   },
   "source": [
    "Alright, it looks like all of the layers in the `efficientnetb0` layer are frozen. We can confirm this using the `trainable_variables` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgSn6yzywHd7",
    "outputId": "573c99b9-26e1-467c-c430-8ff4ca4a7377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# How many layers are trainable in our base model?\n",
    "print(len(model_2.layers[2].trainable_variables)) # layer at index 2 is the EfficientNetB0 layer (the base model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2cydE75i2OC"
   },
   "source": [
    "This is the same as our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEFHfoVti1d4",
    "outputId": "d1e020e7-ff06-4796-b9e1-50c14bae4787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(base_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n1qhcK4jFwx"
   },
   "source": [
    "We can even check layer by layer to see if the they're trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGSvBXM7jKIm",
    "outputId": "3cfd3778-8547-4c04-c0f4-5d3e8ecaab65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3 False\n",
      "1 rescaling_2 False\n",
      "2 normalization_2 False\n",
      "3 stem_conv_pad False\n",
      "4 stem_conv False\n",
      "5 stem_bn False\n",
      "6 stem_activation False\n",
      "7 block1a_dwconv False\n",
      "8 block1a_bn False\n",
      "9 block1a_activation False\n",
      "10 block1a_se_squeeze False\n",
      "11 block1a_se_reshape False\n",
      "12 block1a_se_reduce False\n",
      "13 block1a_se_expand False\n",
      "14 block1a_se_excite False\n",
      "15 block1a_project_conv False\n",
      "16 block1a_project_bn False\n",
      "17 block2a_expand_conv False\n",
      "18 block2a_expand_bn False\n",
      "19 block2a_expand_activation False\n",
      "20 block2a_dwconv_pad False\n",
      "21 block2a_dwconv False\n",
      "22 block2a_bn False\n",
      "23 block2a_activation False\n",
      "24 block2a_se_squeeze False\n",
      "25 block2a_se_reshape False\n",
      "26 block2a_se_reduce False\n",
      "27 block2a_se_expand False\n",
      "28 block2a_se_excite False\n",
      "29 block2a_project_conv False\n",
      "30 block2a_project_bn False\n",
      "31 block2b_expand_conv False\n",
      "32 block2b_expand_bn False\n",
      "33 block2b_expand_activation False\n",
      "34 block2b_dwconv False\n",
      "35 block2b_bn False\n",
      "36 block2b_activation False\n",
      "37 block2b_se_squeeze False\n",
      "38 block2b_se_reshape False\n",
      "39 block2b_se_reduce False\n",
      "40 block2b_se_expand False\n",
      "41 block2b_se_excite False\n",
      "42 block2b_project_conv False\n",
      "43 block2b_project_bn False\n",
      "44 block2b_drop False\n",
      "45 block2b_add False\n",
      "46 block3a_expand_conv False\n",
      "47 block3a_expand_bn False\n",
      "48 block3a_expand_activation False\n",
      "49 block3a_dwconv_pad False\n",
      "50 block3a_dwconv False\n",
      "51 block3a_bn False\n",
      "52 block3a_activation False\n",
      "53 block3a_se_squeeze False\n",
      "54 block3a_se_reshape False\n",
      "55 block3a_se_reduce False\n",
      "56 block3a_se_expand False\n",
      "57 block3a_se_excite False\n",
      "58 block3a_project_conv False\n",
      "59 block3a_project_bn False\n",
      "60 block3b_expand_conv False\n",
      "61 block3b_expand_bn False\n",
      "62 block3b_expand_activation False\n",
      "63 block3b_dwconv False\n",
      "64 block3b_bn False\n",
      "65 block3b_activation False\n",
      "66 block3b_se_squeeze False\n",
      "67 block3b_se_reshape False\n",
      "68 block3b_se_reduce False\n",
      "69 block3b_se_expand False\n",
      "70 block3b_se_excite False\n",
      "71 block3b_project_conv False\n",
      "72 block3b_project_bn False\n",
      "73 block3b_drop False\n",
      "74 block3b_add False\n",
      "75 block4a_expand_conv False\n",
      "76 block4a_expand_bn False\n",
      "77 block4a_expand_activation False\n",
      "78 block4a_dwconv_pad False\n",
      "79 block4a_dwconv False\n",
      "80 block4a_bn False\n",
      "81 block4a_activation False\n",
      "82 block4a_se_squeeze False\n",
      "83 block4a_se_reshape False\n",
      "84 block4a_se_reduce False\n",
      "85 block4a_se_expand False\n",
      "86 block4a_se_excite False\n",
      "87 block4a_project_conv False\n",
      "88 block4a_project_bn False\n",
      "89 block4b_expand_conv False\n",
      "90 block4b_expand_bn False\n",
      "91 block4b_expand_activation False\n",
      "92 block4b_dwconv False\n",
      "93 block4b_bn False\n",
      "94 block4b_activation False\n",
      "95 block4b_se_squeeze False\n",
      "96 block4b_se_reshape False\n",
      "97 block4b_se_reduce False\n",
      "98 block4b_se_expand False\n",
      "99 block4b_se_excite False\n",
      "100 block4b_project_conv False\n",
      "101 block4b_project_bn False\n",
      "102 block4b_drop False\n",
      "103 block4b_add False\n",
      "104 block4c_expand_conv False\n",
      "105 block4c_expand_bn False\n",
      "106 block4c_expand_activation False\n",
      "107 block4c_dwconv False\n",
      "108 block4c_bn False\n",
      "109 block4c_activation False\n",
      "110 block4c_se_squeeze False\n",
      "111 block4c_se_reshape False\n",
      "112 block4c_se_reduce False\n",
      "113 block4c_se_expand False\n",
      "114 block4c_se_excite False\n",
      "115 block4c_project_conv False\n",
      "116 block4c_project_bn False\n",
      "117 block4c_drop False\n",
      "118 block4c_add False\n",
      "119 block5a_expand_conv False\n",
      "120 block5a_expand_bn False\n",
      "121 block5a_expand_activation False\n",
      "122 block5a_dwconv False\n",
      "123 block5a_bn False\n",
      "124 block5a_activation False\n",
      "125 block5a_se_squeeze False\n",
      "126 block5a_se_reshape False\n",
      "127 block5a_se_reduce False\n",
      "128 block5a_se_expand False\n",
      "129 block5a_se_excite False\n",
      "130 block5a_project_conv False\n",
      "131 block5a_project_bn False\n",
      "132 block5b_expand_conv False\n",
      "133 block5b_expand_bn False\n",
      "134 block5b_expand_activation False\n",
      "135 block5b_dwconv False\n",
      "136 block5b_bn False\n",
      "137 block5b_activation False\n",
      "138 block5b_se_squeeze False\n",
      "139 block5b_se_reshape False\n",
      "140 block5b_se_reduce False\n",
      "141 block5b_se_expand False\n",
      "142 block5b_se_excite False\n",
      "143 block5b_project_conv False\n",
      "144 block5b_project_bn False\n",
      "145 block5b_drop False\n",
      "146 block5b_add False\n",
      "147 block5c_expand_conv False\n",
      "148 block5c_expand_bn False\n",
      "149 block5c_expand_activation False\n",
      "150 block5c_dwconv False\n",
      "151 block5c_bn False\n",
      "152 block5c_activation False\n",
      "153 block5c_se_squeeze False\n",
      "154 block5c_se_reshape False\n",
      "155 block5c_se_reduce False\n",
      "156 block5c_se_expand False\n",
      "157 block5c_se_excite False\n",
      "158 block5c_project_conv False\n",
      "159 block5c_project_bn False\n",
      "160 block5c_drop False\n",
      "161 block5c_add False\n",
      "162 block6a_expand_conv False\n",
      "163 block6a_expand_bn False\n",
      "164 block6a_expand_activation False\n",
      "165 block6a_dwconv_pad False\n",
      "166 block6a_dwconv False\n",
      "167 block6a_bn False\n",
      "168 block6a_activation False\n",
      "169 block6a_se_squeeze False\n",
      "170 block6a_se_reshape False\n",
      "171 block6a_se_reduce False\n",
      "172 block6a_se_expand False\n",
      "173 block6a_se_excite False\n",
      "174 block6a_project_conv False\n",
      "175 block6a_project_bn False\n",
      "176 block6b_expand_conv False\n",
      "177 block6b_expand_bn False\n",
      "178 block6b_expand_activation False\n",
      "179 block6b_dwconv False\n",
      "180 block6b_bn False\n",
      "181 block6b_activation False\n",
      "182 block6b_se_squeeze False\n",
      "183 block6b_se_reshape False\n",
      "184 block6b_se_reduce False\n",
      "185 block6b_se_expand False\n",
      "186 block6b_se_excite False\n",
      "187 block6b_project_conv False\n",
      "188 block6b_project_bn False\n",
      "189 block6b_drop False\n",
      "190 block6b_add False\n",
      "191 block6c_expand_conv False\n",
      "192 block6c_expand_bn False\n",
      "193 block6c_expand_activation False\n",
      "194 block6c_dwconv False\n",
      "195 block6c_bn False\n",
      "196 block6c_activation False\n",
      "197 block6c_se_squeeze False\n",
      "198 block6c_se_reshape False\n",
      "199 block6c_se_reduce False\n",
      "200 block6c_se_expand False\n",
      "201 block6c_se_excite False\n",
      "202 block6c_project_conv False\n",
      "203 block6c_project_bn False\n",
      "204 block6c_drop False\n",
      "205 block6c_add False\n",
      "206 block6d_expand_conv False\n",
      "207 block6d_expand_bn False\n",
      "208 block6d_expand_activation False\n",
      "209 block6d_dwconv False\n",
      "210 block6d_bn False\n",
      "211 block6d_activation False\n",
      "212 block6d_se_squeeze False\n",
      "213 block6d_se_reshape False\n",
      "214 block6d_se_reduce False\n",
      "215 block6d_se_expand False\n",
      "216 block6d_se_excite False\n",
      "217 block6d_project_conv False\n",
      "218 block6d_project_bn False\n",
      "219 block6d_drop False\n",
      "220 block6d_add False\n",
      "221 block7a_expand_conv False\n",
      "222 block7a_expand_bn False\n",
      "223 block7a_expand_activation False\n",
      "224 block7a_dwconv False\n",
      "225 block7a_bn False\n",
      "226 block7a_activation False\n",
      "227 block7a_se_squeeze False\n",
      "228 block7a_se_reshape False\n",
      "229 block7a_se_reduce False\n",
      "230 block7a_se_expand False\n",
      "231 block7a_se_excite False\n",
      "232 block7a_project_conv False\n",
      "233 block7a_project_bn False\n",
      "234 top_conv False\n",
      "235 top_bn False\n",
      "236 top_activation False\n"
     ]
    }
   ],
   "source": [
    "# Check which layers are tuneable (trainable)\n",
    "for layer_number, layer in enumerate(base_model.layers):\n",
    "  print(layer_number, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT7tYrL8iNjt"
   },
   "source": [
    "Beautiful. This is exactly what we're after. \n",
    "\n",
    "Now to fine-tune the base model to our own data, we're going to unfreeze the top 10 layers and continue training our model for another 5 epochs.\n",
    "\n",
    "This means all of the base model's layers except for the last 10 will remain frozen and untrainable. And the weights in the remaining unfrozen layers will be updated during training.\n",
    "\n",
    "Ideally, we should see the model's performance improve.\n",
    "\n",
    "> 🤔 **Question:** How many layers should you unfreeze when training?\n",
    "\n",
    "There's no set rule for this. You could unfreeze every layer in the pretrained model or you could try unfreezing one layer at a time. Best to experiment with different amounts of unfreezing and fine-tuning to see what happens. Generally, the less data you have, the less layers you want to unfreeze and the more gradually you want to fine-tune.\n",
    "\n",
    "> 📖 **Resource:** The [ULMFiT (Universal Language Model Fine-tuning for Text Classification) paper](https://arxiv.org/abs/1801.06146) has a great series of experiments on fine-tuning models.\n",
    "\n",
    "To begin fine-tuning, we'll unfreeze the entire base model by setting its `trainable` attribute to `True`. Then we'll refreeze every layer in the base model except for the last 10 by looping through them and setting their `trainable` attribute to `False`. Finally, we'll recompile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RN8aH82i__f"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except for the\n",
    "for layer in base_model.layers[:-10]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Recompile the model (always recompile after any adjustments to a model)\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.0001), # lr is 10x lower than before for fine-tuning\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSVFvTiHjYP-"
   },
   "source": [
    "Wonderful, now let's check which layers of the pretrained model are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyKYo9yB7Xjq",
    "outputId": "8e43382c-bdb5-4efa-d1bb-ee32e1cb1308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3 False\n",
      "1 rescaling_2 False\n",
      "2 normalization_2 False\n",
      "3 stem_conv_pad False\n",
      "4 stem_conv False\n",
      "5 stem_bn False\n",
      "6 stem_activation False\n",
      "7 block1a_dwconv False\n",
      "8 block1a_bn False\n",
      "9 block1a_activation False\n",
      "10 block1a_se_squeeze False\n",
      "11 block1a_se_reshape False\n",
      "12 block1a_se_reduce False\n",
      "13 block1a_se_expand False\n",
      "14 block1a_se_excite False\n",
      "15 block1a_project_conv False\n",
      "16 block1a_project_bn False\n",
      "17 block2a_expand_conv False\n",
      "18 block2a_expand_bn False\n",
      "19 block2a_expand_activation False\n",
      "20 block2a_dwconv_pad False\n",
      "21 block2a_dwconv False\n",
      "22 block2a_bn False\n",
      "23 block2a_activation False\n",
      "24 block2a_se_squeeze False\n",
      "25 block2a_se_reshape False\n",
      "26 block2a_se_reduce False\n",
      "27 block2a_se_expand False\n",
      "28 block2a_se_excite False\n",
      "29 block2a_project_conv False\n",
      "30 block2a_project_bn False\n",
      "31 block2b_expand_conv False\n",
      "32 block2b_expand_bn False\n",
      "33 block2b_expand_activation False\n",
      "34 block2b_dwconv False\n",
      "35 block2b_bn False\n",
      "36 block2b_activation False\n",
      "37 block2b_se_squeeze False\n",
      "38 block2b_se_reshape False\n",
      "39 block2b_se_reduce False\n",
      "40 block2b_se_expand False\n",
      "41 block2b_se_excite False\n",
      "42 block2b_project_conv False\n",
      "43 block2b_project_bn False\n",
      "44 block2b_drop False\n",
      "45 block2b_add False\n",
      "46 block3a_expand_conv False\n",
      "47 block3a_expand_bn False\n",
      "48 block3a_expand_activation False\n",
      "49 block3a_dwconv_pad False\n",
      "50 block3a_dwconv False\n",
      "51 block3a_bn False\n",
      "52 block3a_activation False\n",
      "53 block3a_se_squeeze False\n",
      "54 block3a_se_reshape False\n",
      "55 block3a_se_reduce False\n",
      "56 block3a_se_expand False\n",
      "57 block3a_se_excite False\n",
      "58 block3a_project_conv False\n",
      "59 block3a_project_bn False\n",
      "60 block3b_expand_conv False\n",
      "61 block3b_expand_bn False\n",
      "62 block3b_expand_activation False\n",
      "63 block3b_dwconv False\n",
      "64 block3b_bn False\n",
      "65 block3b_activation False\n",
      "66 block3b_se_squeeze False\n",
      "67 block3b_se_reshape False\n",
      "68 block3b_se_reduce False\n",
      "69 block3b_se_expand False\n",
      "70 block3b_se_excite False\n",
      "71 block3b_project_conv False\n",
      "72 block3b_project_bn False\n",
      "73 block3b_drop False\n",
      "74 block3b_add False\n",
      "75 block4a_expand_conv False\n",
      "76 block4a_expand_bn False\n",
      "77 block4a_expand_activation False\n",
      "78 block4a_dwconv_pad False\n",
      "79 block4a_dwconv False\n",
      "80 block4a_bn False\n",
      "81 block4a_activation False\n",
      "82 block4a_se_squeeze False\n",
      "83 block4a_se_reshape False\n",
      "84 block4a_se_reduce False\n",
      "85 block4a_se_expand False\n",
      "86 block4a_se_excite False\n",
      "87 block4a_project_conv False\n",
      "88 block4a_project_bn False\n",
      "89 block4b_expand_conv False\n",
      "90 block4b_expand_bn False\n",
      "91 block4b_expand_activation False\n",
      "92 block4b_dwconv False\n",
      "93 block4b_bn False\n",
      "94 block4b_activation False\n",
      "95 block4b_se_squeeze False\n",
      "96 block4b_se_reshape False\n",
      "97 block4b_se_reduce False\n",
      "98 block4b_se_expand False\n",
      "99 block4b_se_excite False\n",
      "100 block4b_project_conv False\n",
      "101 block4b_project_bn False\n",
      "102 block4b_drop False\n",
      "103 block4b_add False\n",
      "104 block4c_expand_conv False\n",
      "105 block4c_expand_bn False\n",
      "106 block4c_expand_activation False\n",
      "107 block4c_dwconv False\n",
      "108 block4c_bn False\n",
      "109 block4c_activation False\n",
      "110 block4c_se_squeeze False\n",
      "111 block4c_se_reshape False\n",
      "112 block4c_se_reduce False\n",
      "113 block4c_se_expand False\n",
      "114 block4c_se_excite False\n",
      "115 block4c_project_conv False\n",
      "116 block4c_project_bn False\n",
      "117 block4c_drop False\n",
      "118 block4c_add False\n",
      "119 block5a_expand_conv False\n",
      "120 block5a_expand_bn False\n",
      "121 block5a_expand_activation False\n",
      "122 block5a_dwconv False\n",
      "123 block5a_bn False\n",
      "124 block5a_activation False\n",
      "125 block5a_se_squeeze False\n",
      "126 block5a_se_reshape False\n",
      "127 block5a_se_reduce False\n",
      "128 block5a_se_expand False\n",
      "129 block5a_se_excite False\n",
      "130 block5a_project_conv False\n",
      "131 block5a_project_bn False\n",
      "132 block5b_expand_conv False\n",
      "133 block5b_expand_bn False\n",
      "134 block5b_expand_activation False\n",
      "135 block5b_dwconv False\n",
      "136 block5b_bn False\n",
      "137 block5b_activation False\n",
      "138 block5b_se_squeeze False\n",
      "139 block5b_se_reshape False\n",
      "140 block5b_se_reduce False\n",
      "141 block5b_se_expand False\n",
      "142 block5b_se_excite False\n",
      "143 block5b_project_conv False\n",
      "144 block5b_project_bn False\n",
      "145 block5b_drop False\n",
      "146 block5b_add False\n",
      "147 block5c_expand_conv False\n",
      "148 block5c_expand_bn False\n",
      "149 block5c_expand_activation False\n",
      "150 block5c_dwconv False\n",
      "151 block5c_bn False\n",
      "152 block5c_activation False\n",
      "153 block5c_se_squeeze False\n",
      "154 block5c_se_reshape False\n",
      "155 block5c_se_reduce False\n",
      "156 block5c_se_expand False\n",
      "157 block5c_se_excite False\n",
      "158 block5c_project_conv False\n",
      "159 block5c_project_bn False\n",
      "160 block5c_drop False\n",
      "161 block5c_add False\n",
      "162 block6a_expand_conv False\n",
      "163 block6a_expand_bn False\n",
      "164 block6a_expand_activation False\n",
      "165 block6a_dwconv_pad False\n",
      "166 block6a_dwconv False\n",
      "167 block6a_bn False\n",
      "168 block6a_activation False\n",
      "169 block6a_se_squeeze False\n",
      "170 block6a_se_reshape False\n",
      "171 block6a_se_reduce False\n",
      "172 block6a_se_expand False\n",
      "173 block6a_se_excite False\n",
      "174 block6a_project_conv False\n",
      "175 block6a_project_bn False\n",
      "176 block6b_expand_conv False\n",
      "177 block6b_expand_bn False\n",
      "178 block6b_expand_activation False\n",
      "179 block6b_dwconv False\n",
      "180 block6b_bn False\n",
      "181 block6b_activation False\n",
      "182 block6b_se_squeeze False\n",
      "183 block6b_se_reshape False\n",
      "184 block6b_se_reduce False\n",
      "185 block6b_se_expand False\n",
      "186 block6b_se_excite False\n",
      "187 block6b_project_conv False\n",
      "188 block6b_project_bn False\n",
      "189 block6b_drop False\n",
      "190 block6b_add False\n",
      "191 block6c_expand_conv False\n",
      "192 block6c_expand_bn False\n",
      "193 block6c_expand_activation False\n",
      "194 block6c_dwconv False\n",
      "195 block6c_bn False\n",
      "196 block6c_activation False\n",
      "197 block6c_se_squeeze False\n",
      "198 block6c_se_reshape False\n",
      "199 block6c_se_reduce False\n",
      "200 block6c_se_expand False\n",
      "201 block6c_se_excite False\n",
      "202 block6c_project_conv False\n",
      "203 block6c_project_bn False\n",
      "204 block6c_drop False\n",
      "205 block6c_add False\n",
      "206 block6d_expand_conv False\n",
      "207 block6d_expand_bn False\n",
      "208 block6d_expand_activation False\n",
      "209 block6d_dwconv False\n",
      "210 block6d_bn False\n",
      "211 block6d_activation False\n",
      "212 block6d_se_squeeze False\n",
      "213 block6d_se_reshape False\n",
      "214 block6d_se_reduce False\n",
      "215 block6d_se_expand False\n",
      "216 block6d_se_excite False\n",
      "217 block6d_project_conv False\n",
      "218 block6d_project_bn False\n",
      "219 block6d_drop False\n",
      "220 block6d_add False\n",
      "221 block7a_expand_conv False\n",
      "222 block7a_expand_bn False\n",
      "223 block7a_expand_activation False\n",
      "224 block7a_dwconv False\n",
      "225 block7a_bn False\n",
      "226 block7a_activation False\n",
      "227 block7a_se_squeeze True\n",
      "228 block7a_se_reshape True\n",
      "229 block7a_se_reduce True\n",
      "230 block7a_se_expand True\n",
      "231 block7a_se_excite True\n",
      "232 block7a_project_conv True\n",
      "233 block7a_project_bn True\n",
      "234 top_conv True\n",
      "235 top_bn True\n",
      "236 top_activation True\n"
     ]
    }
   ],
   "source": [
    "# Check which layers are tuneable (trainable)\n",
    "for layer_number, layer in enumerate(base_model.layers):\n",
    "  print(layer_number, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHvSykH7jfRK"
   },
   "source": [
    "Nice! It seems all layers except for the last 10 are frozen and untrainable. This means only the last 10 layers of the base model along with the output layer will have their weights updated during training.\n",
    "\n",
    "> 🤔 **Question:** Why did we recompile the model?\n",
    "\n",
    "Every time you make a change to your models, you need to recompile them.\n",
    "\n",
    "In our case, we're using the exact same loss, optimizer and metrics as before, except this time the learning rate for our optimizer will be 10x smaller than before (0.0001 instead of Adam's default of 0.001).\n",
    "\n",
    "We do this so the model doesn't try to overwrite the existing weights in the pretrained model too fast. In other words, we want learning to be more gradual.\n",
    "\n",
    "> 🔑 **Note:** There's no set standard for setting the learning rate during fine-tuning, though reductions of [2.6x-10x+ seem to work well in practice](https://arxiv.org/abs/1801.06146).\n",
    "\n",
    "How many trainable variables do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbgcdnJIjicF",
    "outputId": "8b93480d-4fe7-4655-ff64-0175d1090ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(model_2.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5wKmzBeuty7"
   },
   "source": [
    "Wonderful, it looks like our model has a total of 10 trainable variables, the last 10 layers of the base model and the weight and bias parameters of the Dense output layer.\n",
    "\n",
    "Time to fine-tune!\n",
    "\n",
    "We're going to continue training on from where our previous model finished. Since it trained for 5 epochs, our fine-tuning will begin on the epoch 5 and continue for another 5 epochs.\n",
    "\n",
    "To do this, we can use the `initial_epoch` parameter of the [`fit()`](https://keras.rstudio.com/reference/fit.html) method. We'll pass it the last epoch of the previous model's training history (`history_10_percent_data_aug.epoch[-1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uH6XM4ENjfRn",
    "outputId": "e873e2bb-77b2-43b8-95cb-e1c658484c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: transfer_learning/10_percent_fine_tune_last_10/20210216-022051\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 15s 439ms/step - loss: 0.6963 - accuracy: 0.8062 - val_loss: 0.6032 - val_accuracy: 0.8043\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 8s 329ms/step - loss: 0.5747 - accuracy: 0.8390 - val_loss: 0.5580 - val_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 8s 319ms/step - loss: 0.4972 - accuracy: 0.8458 - val_loss: 0.5543 - val_accuracy: 0.8240\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 8s 323ms/step - loss: 0.4262 - accuracy: 0.8814 - val_loss: 0.5403 - val_accuracy: 0.8191\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 8s 300ms/step - loss: 0.4234 - accuracy: 0.8855 - val_loss: 0.5262 - val_accuracy: 0.8322\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 8s 305ms/step - loss: 0.3665 - accuracy: 0.9056 - val_loss: 0.5218 - val_accuracy: 0.8322\n"
     ]
    }
   ],
   "source": [
    "# Fine tune for another 5 epochs\n",
    "fine_tune_epochs = initial_epochs + 5\n",
    "\n",
    "# Refit the model (same as model_2 except with more trainable layers)\n",
    "history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
    "                                               epochs=fine_tune_epochs,\n",
    "                                               validation_data=test_data,\n",
    "                                               initial_epoch=history_10_percent_data_aug.epoch[-1], # start from previous last epoch\n",
    "                                               validation_steps=int(0.25 * len(test_data)),\n",
    "                                               callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_fine_tune_last_10\")]) # name experiment appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lrTVkS5wJT7"
   },
   "source": [
    "> 🔑 **Note:** Fine-tuning usually takes far longer per epoch than feature extraction (due to updating more weights throughout a network).\n",
    "\n",
    "Ho ho, looks like our model has gained a few percentage points of accuracy! Let's evalaute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbFET5Pj8U3F",
    "outputId": "021196e6-cac6-4b8d-92da-83f97d967f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 10s 117ms/step - loss: 0.4870 - accuracy: 0.8388\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "results_fine_tune_10_percent = model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDnoGFCbwRe4"
   },
   "source": [
    "Remember, the results from evaluating the model might be slightly different to the outputs from training since during training we only evaluate on 25% of the test data.\n",
    "\n",
    "Alright, we need a way to evaluate our model's performance before and after fine-tuning. How about we write a function to compare the before and after?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8du2ihQNkm4W"
   },
   "outputs": [],
   "source": [
    "def compare_historys(original_history, new_history, initial_epochs=5):\n",
    "    \"\"\"\n",
    "    Compares two model history objects.\n",
    "    \"\"\"\n",
    "    # Get original history measurements\n",
    "    acc = original_history.history[\"accuracy\"]\n",
    "    loss = original_history.history[\"loss\"]\n",
    "\n",
    "    print(len(acc))\n",
    "\n",
    "    val_acc = original_history.history[\"val_accuracy\"]\n",
    "    val_loss = original_history.history[\"val_loss\"]\n",
    "\n",
    "    # Combine original history with new history\n",
    "    total_acc = acc + new_history.history[\"accuracy\"]\n",
    "    total_loss = loss + new_history.history[\"loss\"]\n",
    "\n",
    "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
    "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
    "\n",
    "    print(len(total_acc))\n",
    "    print(total_acc)\n",
    "\n",
    "    # Make plots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_acc, label='Training Accuracy')\n",
    "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
    "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(total_loss, label='Training Loss')\n",
    "    plt.plot(total_val_loss, label='Validation Loss')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
    "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-SsaN7GxNOJ"
   },
   "source": [
    "This is where saving the history variables of our model training comes in handy. Let's see what happened after fine-tuning the last 10 layers of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "BJANCd2e5Olt",
    "outputId": "aadf20cf-a4bf-4d02-c865-5d138ce1c006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "[0.3293333351612091, 0.6453333497047424, 0.7333333492279053, 0.7693333625793457, 0.7946666479110718, 0.8026666641235352, 0.8320000171661377, 0.846666693687439, 0.8613333106040955, 0.8893333077430725, 0.8893333077430725]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_historys(original_history=history_10_percent_data_aug, \n",
    "                 new_history=history_fine_10_percent_data_aug, \n",
    "                 initial_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3FiQuJoxbOq"
   },
   "source": [
    "Alright, alright, seems like the curves are heading in the right direction after fine-tuning. But remember, it should be noted that fine-tuning usually works best with larger amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJMW2KKiqFsa"
   },
   "source": [
    "## Model 4: Fine-tuning an existing model all of the data\n",
    "\n",
    "Enough talk about how fine-tuning a model usually works with more data, let's try it out.\n",
    "\n",
    "We'll start by downloading the full version of our 10 food classes dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJp-39zmLgFO",
    "outputId": "41741808-c29a-4352-ef23-05827891bcd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-16 02:48:30--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.164.144, 172.253.115.128, 172.253.63.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.164.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 519183241 (495M) [application/zip]\n",
      "Saving to: ‘10_food_classes_all_data.zip’\n",
      "\n",
      "10_food_classes_all 100%[===================>] 495.13M   124MB/s    in 4.1s    \n",
      "\n",
      "2021-02-16 02:48:35 (121 MB/s) - ‘10_food_classes_all_data.zip’ saved [519183241/519183241]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip 10 classes of data with all images\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip \n",
    "unzip_data(\"10_food_classes_all_data.zip\")\n",
    "\n",
    "# Setup data directories\n",
    "train_dir = \"10_food_classes_all_data/train/\"\n",
    "test_dir = \"10_food_classes_all_data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUhaT5repEp0",
    "outputId": "53695c66-5d13-4ee4-9e25-10066ac5f1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '10_food_classes_all_data'.\n",
      "There are 10 directories and 0 images in '10_food_classes_all_data/train'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/ice_cream'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/ramen'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/chicken_wings'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/pizza'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/steak'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/fried_rice'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/hamburger'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/grilled_salmon'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/sushi'.\n",
      "There are 0 directories and 750 images in '10_food_classes_all_data/train/chicken_curry'.\n",
      "There are 10 directories and 0 images in '10_food_classes_all_data/test'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/ice_cream'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/ramen'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/chicken_wings'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/pizza'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/steak'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/fried_rice'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/hamburger'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/grilled_salmon'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/sushi'.\n",
      "There are 0 directories and 250 images in '10_food_classes_all_data/test/chicken_curry'.\n"
     ]
    }
   ],
   "source": [
    "# How many images are we working with now?\n",
    "walk_through_dir(\"10_food_classes_all_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uwDRLJ9z_bZ"
   },
   "source": [
    "And now we'll turn the images into tensors datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-bF8cAKLzVt",
    "outputId": "8603a5da-c161-42b1-8134-bd1772510120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 files belonging to 10 classes.\n",
      "Found 2500 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setup data inputs\n",
    "import tensorflow as tf\n",
    "IMG_SIZE = (224, 224)\n",
    "train_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
    "                                                                                 label_mode=\"categorical\",\n",
    "                                                                                 image_size=IMG_SIZE)\n",
    "\n",
    "# Note: this is the same test dataset we've been using for the previous modelling experiments\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
    "                                                                label_mode=\"categorical\",\n",
    "                                                                image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdavTttb_H9u"
   },
   "source": [
    "Oh this is looking good. We've got 10x more images in of the training classes to work with.\n",
    "\n",
    "The **test dataset is the same** we've been using for our previous experiments.\n",
    "\n",
    "As it is now, our `model_2` has been fine-tuned on 10 percent of the data, so to begin fine-tuning on all of the data and keep our experiments consistent, we need to revert it back to the weights we checkpointed after 5 epochs of feature-extraction.\n",
    "\n",
    "To demonstrate this, we'll first evaluate the current `model_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjrkv4n-428G",
    "outputId": "e5794a93-fcc2-4813-fa9c-203358a9b68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 10s 118ms/step - loss: 0.4870 - accuracy: 0.8388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4869591295719147, 0.8388000130653381]"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model (this is the fine-tuned 10 percent of data version)\n",
    "model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz-AAATk1pUP"
   },
   "source": [
    "These are the same values as `results_fine_tune_10_percent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D0wLFb01kaJ",
    "outputId": "ab7ec355-5961-4124-e2f3-623f161f4bc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48695918917655945, 0.8388000130653381]"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fine_tune_10_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow6pvNQr1z0u"
   },
   "source": [
    "Now we'll revert the model back to the saved weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqVZlZzVHFys",
    "outputId": "c627895d-f210-4dc2-a67b-29f54bec5e14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcd9da1e4a8>"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from checkpoint, that way we can fine-tune from the same stage the 10 percent data model was fine-tuned from\n",
    "model_2.load_weights(checkpoint_path) # revert model back to saved weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95DM8uzT138v"
   },
   "source": [
    "And the results should be the same as `results_10_percent_data_aug`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wET1oWQC0yc2",
    "outputId": "fedf7481-bf03-4799-902d-8cba5c066ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 10s 117ms/step - loss: 0.7047 - accuracy: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7046542763710022, 0.8080000281333923]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After loading the weights, this should have gone down (no fine-tuning)\n",
    "model_2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "246x0CejyN8b",
    "outputId": "1f7fc091-3d50-489c-b7bd-ca12ed6d650e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7046541571617126, 0.8080000281333923]"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if the above two results are the same (they should be)\n",
    "results_10_percent_data_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dGc9MML2Fn0"
   },
   "source": [
    "Alright, the previous steps might seem quite confusing but all we've done is:\n",
    "1. Trained a feature extraction transfer learning model for 5 epochs on 10% of the data (with all base model layers frozen) and saved the model's weights using `ModelCheckpoint`.\n",
    "2. Fine-tuned the same model on the same 10% of the data for a further 5 epochs with the top 10 layers of the base model unfrozen.\n",
    "3. Saved the results and training logs each time.\n",
    "4. Reloaded the model from 1 to do the same steps as 2 but with all of the data.\n",
    "\n",
    "The same steps as 2?\n",
    "\n",
    "Yeah, we're going to fine-tune the last 10 layers of the base model with the full dataset for another 5 epochs but first let's remind ourselves which layers are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5MWo0Mu6WYl",
    "outputId": "e573b4a0-b6ba-441f-a3c9-c5c5a01b59cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_layer True\n",
      "1 data_augmentation True\n",
      "2 efficientnetb0 True\n",
      "3 global_average_pooling_layer True\n",
      "4 output_layer True\n"
     ]
    }
   ],
   "source": [
    "# Check which layers are tuneable in the whole model\n",
    "for layer_number, layer in enumerate(model_2.layers):\n",
    "  print(layer_number, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGFZC_fz3M2i"
   },
   "source": [
    "Can we get a little more specific?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojbNIiUV8oc2",
    "outputId": "cad2ac02-6c59-4430-cae8-f0c5d4528d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3 False\n",
      "1 rescaling_2 False\n",
      "2 normalization_2 False\n",
      "3 stem_conv_pad False\n",
      "4 stem_conv False\n",
      "5 stem_bn False\n",
      "6 stem_activation False\n",
      "7 block1a_dwconv False\n",
      "8 block1a_bn False\n",
      "9 block1a_activation False\n",
      "10 block1a_se_squeeze False\n",
      "11 block1a_se_reshape False\n",
      "12 block1a_se_reduce False\n",
      "13 block1a_se_expand False\n",
      "14 block1a_se_excite False\n",
      "15 block1a_project_conv False\n",
      "16 block1a_project_bn False\n",
      "17 block2a_expand_conv False\n",
      "18 block2a_expand_bn False\n",
      "19 block2a_expand_activation False\n",
      "20 block2a_dwconv_pad False\n",
      "21 block2a_dwconv False\n",
      "22 block2a_bn False\n",
      "23 block2a_activation False\n",
      "24 block2a_se_squeeze False\n",
      "25 block2a_se_reshape False\n",
      "26 block2a_se_reduce False\n",
      "27 block2a_se_expand False\n",
      "28 block2a_se_excite False\n",
      "29 block2a_project_conv False\n",
      "30 block2a_project_bn False\n",
      "31 block2b_expand_conv False\n",
      "32 block2b_expand_bn False\n",
      "33 block2b_expand_activation False\n",
      "34 block2b_dwconv False\n",
      "35 block2b_bn False\n",
      "36 block2b_activation False\n",
      "37 block2b_se_squeeze False\n",
      "38 block2b_se_reshape False\n",
      "39 block2b_se_reduce False\n",
      "40 block2b_se_expand False\n",
      "41 block2b_se_excite False\n",
      "42 block2b_project_conv False\n",
      "43 block2b_project_bn False\n",
      "44 block2b_drop False\n",
      "45 block2b_add False\n",
      "46 block3a_expand_conv False\n",
      "47 block3a_expand_bn False\n",
      "48 block3a_expand_activation False\n",
      "49 block3a_dwconv_pad False\n",
      "50 block3a_dwconv False\n",
      "51 block3a_bn False\n",
      "52 block3a_activation False\n",
      "53 block3a_se_squeeze False\n",
      "54 block3a_se_reshape False\n",
      "55 block3a_se_reduce False\n",
      "56 block3a_se_expand False\n",
      "57 block3a_se_excite False\n",
      "58 block3a_project_conv False\n",
      "59 block3a_project_bn False\n",
      "60 block3b_expand_conv False\n",
      "61 block3b_expand_bn False\n",
      "62 block3b_expand_activation False\n",
      "63 block3b_dwconv False\n",
      "64 block3b_bn False\n",
      "65 block3b_activation False\n",
      "66 block3b_se_squeeze False\n",
      "67 block3b_se_reshape False\n",
      "68 block3b_se_reduce False\n",
      "69 block3b_se_expand False\n",
      "70 block3b_se_excite False\n",
      "71 block3b_project_conv False\n",
      "72 block3b_project_bn False\n",
      "73 block3b_drop False\n",
      "74 block3b_add False\n",
      "75 block4a_expand_conv False\n",
      "76 block4a_expand_bn False\n",
      "77 block4a_expand_activation False\n",
      "78 block4a_dwconv_pad False\n",
      "79 block4a_dwconv False\n",
      "80 block4a_bn False\n",
      "81 block4a_activation False\n",
      "82 block4a_se_squeeze False\n",
      "83 block4a_se_reshape False\n",
      "84 block4a_se_reduce False\n",
      "85 block4a_se_expand False\n",
      "86 block4a_se_excite False\n",
      "87 block4a_project_conv False\n",
      "88 block4a_project_bn False\n",
      "89 block4b_expand_conv False\n",
      "90 block4b_expand_bn False\n",
      "91 block4b_expand_activation False\n",
      "92 block4b_dwconv False\n",
      "93 block4b_bn False\n",
      "94 block4b_activation False\n",
      "95 block4b_se_squeeze False\n",
      "96 block4b_se_reshape False\n",
      "97 block4b_se_reduce False\n",
      "98 block4b_se_expand False\n",
      "99 block4b_se_excite False\n",
      "100 block4b_project_conv False\n",
      "101 block4b_project_bn False\n",
      "102 block4b_drop False\n",
      "103 block4b_add False\n",
      "104 block4c_expand_conv False\n",
      "105 block4c_expand_bn False\n",
      "106 block4c_expand_activation False\n",
      "107 block4c_dwconv False\n",
      "108 block4c_bn False\n",
      "109 block4c_activation False\n",
      "110 block4c_se_squeeze False\n",
      "111 block4c_se_reshape False\n",
      "112 block4c_se_reduce False\n",
      "113 block4c_se_expand False\n",
      "114 block4c_se_excite False\n",
      "115 block4c_project_conv False\n",
      "116 block4c_project_bn False\n",
      "117 block4c_drop False\n",
      "118 block4c_add False\n",
      "119 block5a_expand_conv False\n",
      "120 block5a_expand_bn False\n",
      "121 block5a_expand_activation False\n",
      "122 block5a_dwconv False\n",
      "123 block5a_bn False\n",
      "124 block5a_activation False\n",
      "125 block5a_se_squeeze False\n",
      "126 block5a_se_reshape False\n",
      "127 block5a_se_reduce False\n",
      "128 block5a_se_expand False\n",
      "129 block5a_se_excite False\n",
      "130 block5a_project_conv False\n",
      "131 block5a_project_bn False\n",
      "132 block5b_expand_conv False\n",
      "133 block5b_expand_bn False\n",
      "134 block5b_expand_activation False\n",
      "135 block5b_dwconv False\n",
      "136 block5b_bn False\n",
      "137 block5b_activation False\n",
      "138 block5b_se_squeeze False\n",
      "139 block5b_se_reshape False\n",
      "140 block5b_se_reduce False\n",
      "141 block5b_se_expand False\n",
      "142 block5b_se_excite False\n",
      "143 block5b_project_conv False\n",
      "144 block5b_project_bn False\n",
      "145 block5b_drop False\n",
      "146 block5b_add False\n",
      "147 block5c_expand_conv False\n",
      "148 block5c_expand_bn False\n",
      "149 block5c_expand_activation False\n",
      "150 block5c_dwconv False\n",
      "151 block5c_bn False\n",
      "152 block5c_activation False\n",
      "153 block5c_se_squeeze False\n",
      "154 block5c_se_reshape False\n",
      "155 block5c_se_reduce False\n",
      "156 block5c_se_expand False\n",
      "157 block5c_se_excite False\n",
      "158 block5c_project_conv False\n",
      "159 block5c_project_bn False\n",
      "160 block5c_drop False\n",
      "161 block5c_add False\n",
      "162 block6a_expand_conv False\n",
      "163 block6a_expand_bn False\n",
      "164 block6a_expand_activation False\n",
      "165 block6a_dwconv_pad False\n",
      "166 block6a_dwconv False\n",
      "167 block6a_bn False\n",
      "168 block6a_activation False\n",
      "169 block6a_se_squeeze False\n",
      "170 block6a_se_reshape False\n",
      "171 block6a_se_reduce False\n",
      "172 block6a_se_expand False\n",
      "173 block6a_se_excite False\n",
      "174 block6a_project_conv False\n",
      "175 block6a_project_bn False\n",
      "176 block6b_expand_conv False\n",
      "177 block6b_expand_bn False\n",
      "178 block6b_expand_activation False\n",
      "179 block6b_dwconv False\n",
      "180 block6b_bn False\n",
      "181 block6b_activation False\n",
      "182 block6b_se_squeeze False\n",
      "183 block6b_se_reshape False\n",
      "184 block6b_se_reduce False\n",
      "185 block6b_se_expand False\n",
      "186 block6b_se_excite False\n",
      "187 block6b_project_conv False\n",
      "188 block6b_project_bn False\n",
      "189 block6b_drop False\n",
      "190 block6b_add False\n",
      "191 block6c_expand_conv False\n",
      "192 block6c_expand_bn False\n",
      "193 block6c_expand_activation False\n",
      "194 block6c_dwconv False\n",
      "195 block6c_bn False\n",
      "196 block6c_activation False\n",
      "197 block6c_se_squeeze False\n",
      "198 block6c_se_reshape False\n",
      "199 block6c_se_reduce False\n",
      "200 block6c_se_expand False\n",
      "201 block6c_se_excite False\n",
      "202 block6c_project_conv False\n",
      "203 block6c_project_bn False\n",
      "204 block6c_drop False\n",
      "205 block6c_add False\n",
      "206 block6d_expand_conv False\n",
      "207 block6d_expand_bn False\n",
      "208 block6d_expand_activation False\n",
      "209 block6d_dwconv False\n",
      "210 block6d_bn False\n",
      "211 block6d_activation False\n",
      "212 block6d_se_squeeze False\n",
      "213 block6d_se_reshape False\n",
      "214 block6d_se_reduce False\n",
      "215 block6d_se_expand False\n",
      "216 block6d_se_excite False\n",
      "217 block6d_project_conv False\n",
      "218 block6d_project_bn False\n",
      "219 block6d_drop False\n",
      "220 block6d_add False\n",
      "221 block7a_expand_conv False\n",
      "222 block7a_expand_bn False\n",
      "223 block7a_expand_activation False\n",
      "224 block7a_dwconv False\n",
      "225 block7a_bn False\n",
      "226 block7a_activation False\n",
      "227 block7a_se_squeeze True\n",
      "228 block7a_se_reshape True\n",
      "229 block7a_se_reduce True\n",
      "230 block7a_se_expand True\n",
      "231 block7a_se_excite True\n",
      "232 block7a_project_conv True\n",
      "233 block7a_project_bn True\n",
      "234 top_conv True\n",
      "235 top_bn True\n",
      "236 top_activation True\n"
     ]
    }
   ],
   "source": [
    "# Check which layers are tuneable in the base model\n",
    "for layer_number, layer in enumerate(base_model.layers):\n",
    "  print(layer_number, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87Nf4mJn1Nin"
   },
   "source": [
    "Looking good! The last 10 layers are trainable (unfrozen).\n",
    "\n",
    "We've got one more step to do before we can begin fine-tuning.\n",
    "\n",
    "Do you remember what it is?\n",
    "\n",
    "I'll give you a hint. We just reloaded the weights to our model and what do we need to do every time we make a change to our models?\n",
    "\n",
    "Recompile them!\n",
    "\n",
    "This will be just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOGsq2qNHBzc"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(lr=0.0001), # divide learning rate by 10 for fine-tuning\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D4dafTl30Pe"
   },
   "source": [
    "Alright, time to fine-tune on all of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SpZS_khtJaYx",
    "outputId": "ca084bf0-a840-4634-e649-ccf32d9b7dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: transfer_learning/full_10_classes_fine_tune_last_10/20210216-025031\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 49s 190ms/step - loss: 0.7943 - accuracy: 0.7513 - val_loss: 0.4029 - val_accuracy: 0.8635\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 51s 215ms/step - loss: 0.6391 - accuracy: 0.7913 - val_loss: 0.3668 - val_accuracy: 0.8882\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 47s 198ms/step - loss: 0.5564 - accuracy: 0.8220 - val_loss: 0.3237 - val_accuracy: 0.9013\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 46s 192ms/step - loss: 0.5030 - accuracy: 0.8394 - val_loss: 0.3390 - val_accuracy: 0.8832\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 45s 191ms/step - loss: 0.4611 - accuracy: 0.8479 - val_loss: 0.3099 - val_accuracy: 0.8980\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 43s 183ms/step - loss: 0.4507 - accuracy: 0.8557 - val_loss: 0.2903 - val_accuracy: 0.9161\n"
     ]
    }
   ],
   "source": [
    "# Continue to train and fine-tune the model to our data\n",
    "fine_tune_epochs = initial_epochs + 5\n",
    "\n",
    "history_fine_10_classes_full = model_2.fit(train_data_10_classes_full,\n",
    "                                           epochs=fine_tune_epochs,\n",
    "                                           initial_epoch=history_10_percent_data_aug.epoch[-1],\n",
    "                                           validation_data=test_data,\n",
    "                                           validation_steps=int(0.25 * len(test_data)),\n",
    "                                           callbacks=[create_tensorboard_callback(\"transfer_learning\", \"full_10_classes_fine_tune_last_10\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b-eZ2B935AC"
   },
   "source": [
    "> 🔑 **Note:** Training took longer per epoch, but that makes sense because we're using 10x more training data than before.\n",
    "\n",
    "Let's evaluate on all of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHEcLE3tRTlB",
    "outputId": "81300114-0fb9-4745-926d-f6c677dc0d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 10s 118ms/step - loss: 0.3264 - accuracy: 0.8988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32638612389564514, 0.8988000154495239]"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fine_tune_full_data = model_2.evaluate(test_data)\n",
    "results_fine_tune_full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD17oZ-a4H84"
   },
   "source": [
    "Nice! It looks like fine-tuning with all of the data has given our model a boost, how do the training curves look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "QwxgAW_l5Meg",
    "outputId": "db323b5d-0698-42b3-8704-e51ae49f4f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "11\n",
      "[0.3293333351612091, 0.6453333497047424, 0.7333333492279053, 0.7693333625793457, 0.7946666479110718, 0.7675999999046326, 0.8066666722297668, 0.8333333134651184, 0.8478666543960571, 0.8511999845504761, 0.8655999898910522]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How did fine-tuning go with more data?\n",
    "compare_historys(original_history=history_10_percent_data_aug,\n",
    "                 new_history=history_fine_10_classes_full,\n",
    "                 initial_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZvQ-7v55RVR"
   },
   "source": [
    "Looks like that extra data helped! Those curves are looking great. And if we trained for longer, they might even keep improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvElXran4brJ"
   },
   "source": [
    "## Viewing our experiment data on TensorBoard\n",
    "\n",
    "Right now our experimental results are scattered all throughout our notebook. If we want to share them with someone, they'd be getting a bunch of different graphs and metrics... not a fun time.\n",
    "\n",
    "But guess what?\n",
    "\n",
    "Thanks to the TensorBoard callback we made with our helper function `create_tensorflow_callback()`, we've been tracking our modelling experiments the whole time.\n",
    "\n",
    "How about we upload them to TensorBoard.dev and check them out?\n",
    "\n",
    "We can do with the `tensorboard dev upload` command and passing it the directory where our experiments have been logged.\n",
    "\n",
    "> 🔑 **Note:** Remember, whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "u_OCtijuI7gv",
    "outputId": "8c0e4120-e6ed-43ef-e438-26681e8a27cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-17 22:51:36.043126: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Data for the \"graphs\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "Data for the \"histograms\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "Data for the \"hparams\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "Upload started and will continue reading any new data as it's added\n",
      "to the logdir. To stop uploading, press Ctrl-C.\n",
      "\n",
      "View your TensorBoard live at: https://tensorboard.dev/experiment/2O76kw3PQbKl0lByfg5B4w/\n",
      "\n",
      "\u001b[1m[2020-09-17T22:51:37]\u001b[0m Uploader started.\n",
      "\u001b[1m[2020-09-17T22:51:47]\u001b[0m Total uploaded: 128 scalars, 0 tensors, 5 binary objects (9.1 MB)\n",
      "\u001b[2K\u001b[33mListening for new data in logdir...\u001b[0m\n",
      "Done. View your TensorBoard at https://tensorboard.dev/experiment/2O76kw3PQbKl0lByfg5B4w/\n"
     ]
    }
   ],
   "source": [
    "# View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n",
    "# Upload TensorBoard dev records\n",
    "!tensorboard dev upload --logdir ./transfer_learning \\\n",
    "  --name \"Transfer learning experiments\" \\\n",
    "  --description \"A series of different transfer learning experiments with varying amounts of data and fine-tuning\" \\\n",
    "  --one_shot # exits the uploader when upload has finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHrA0duU6txY"
   },
   "source": [
    "Once we've uploaded the results to TensorBoard.dev we get a shareable link we can use to view and compare our experiments and share our results with others if needed.\n",
    "\n",
    "You can view the original versions of the experiments we ran in this notebook here: https://tensorboard.dev/experiment/2O76kw3PQbKl0lByfg5B4w/\n",
    "\n",
    "> 🤔 **Question:** Which model performed the best? Why do you think this is? How did fine-tuning go?\n",
    "\n",
    "To find all of your previous TensorBoard.dev experiments using the command `tensorboard dev list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "dy4FuwhrLKzr",
    "outputId": "b97e1f31-912f-4dfe-f66a-e415be5301d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-17 22:51:48.747476: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Data for the \"graphs\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "Data for the \"histograms\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "Data for the \"hparams\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "https://tensorboard.dev/experiment/2O76kw3PQbKl0lByfg5B4w/\n",
      "\tName                 Transfer learning experiments\n",
      "\tDescription          A series of different transfer learning experiments with varying amounts of data and fine-tuning\n",
      "\tId                   2O76kw3PQbKl0lByfg5B4w\n",
      "\tCreated              2020-09-17 22:51:37 (15 seconds ago)\n",
      "\tUpdated              2020-09-17 22:51:47 (5 seconds ago)\n",
      "\tRuns                 10\n",
      "\tTags                 3\n",
      "\tScalars              128\n",
      "\tTensor bytes         0\n",
      "\tBinary object bytes  9520961\n",
      "https://tensorboard.dev/experiment/73taSKxXQeGPQsNBcVvY3g/\n",
      "\tName                 EfficientNetB0 vs. ResNet50V2\n",
      "\tDescription          Comparing two different TF Hub feature extraction models architectures using 10% of training images\n",
      "\tId                   73taSKxXQeGPQsNBcVvY3g\n",
      "\tCreated              2020-09-14 05:02:48\n",
      "\tUpdated              2020-09-14 05:02:50\n",
      "\tRuns                 4\n",
      "\tTags                 3\n",
      "\tScalars              40\n",
      "\tTensor bytes         0\n",
      "\tBinary object bytes  3402042\n",
      "Total: 2 experiment(s)\n"
     ]
    }
   ],
   "source": [
    "# View previous experiments\n",
    "!tensorboard dev list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icXKqZkn7HRA"
   },
   "source": [
    "And if you want to remove a previous experiment (and delete it from public viewing) you can use the command:\n",
    "\n",
    "```\n",
    "tensorboard dev delete --experiment_id [INSERT_EXPERIMENT_ID_TO_DELETE]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "a4zeQFbF9XLm",
    "outputId": "3fd10503-23d8-4433-9683-30feda49dba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-17 22:51:53.982454: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Data for the \"graphs\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "Data for the \"histograms\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "Data for the \"hparams\" plugin is now uploaded to TensorBoard.dev! Note that uploaded data is public. If you do not want to upload data for this plugin, use the \"--plugins\" command line argument.\n",
      "No such experiment OUbW0O3pRqqQgAphVBxi8Q. Either it never existed or it has already been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Remove previous experiments\n",
    "# !tensorboard dev delete --experiment_id OUbW0O3pRqqQgAphVBxi8Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P76rEQjPdR8m"
   },
   "source": [
    "## 🛠 Exercises\n",
    "\n",
    "1. Write a function to visualize an image from any dataset (train or test file) and any class (e.g. \"steak\", \"pizza\"... etc), visualize it and make a prediction on it using a trained model.\n",
    "2. Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using [`tf.keras.applications.EfficientNetB0`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0) as the base model. Use the [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback to save the weights to file.\n",
    "3. Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go?\n",
    "4. Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvwXqey-dhpT"
   },
   "source": [
    "## 📖 Extra-curriculum\n",
    "\n",
    "* Read the [documentation on data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) in TensorFlow.\n",
    "* Read the [ULMFit paper](https://arxiv.org/abs/1801.06146) (technical) for an introduction to the concept of freezing and unfreezing different layers.\n",
    "* Read up on learning rate scheduling (there's a [TensorFlow callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) for this), how could this influence our model training?\n",
    "  * If you're training for longer, you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill, the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get, the smaller your movements become."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMT2n+0Q+mlu+rts8JfbPTU",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1alsWE48suOuUXaoAJQpo3-kuU2UJYXKR",
   "name": "05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
